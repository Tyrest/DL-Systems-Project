{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9898964",
   "metadata": {},
   "source": [
    "# MNIST Quantization Benchmark\n",
    "\n",
    "This notebook trains a simple MLP on MNIST using the Needle framework (CUDA backend), quantizes it to Int8, and benchmarks the inference speed and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3682524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n",
      "Using device: cuda()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Set backend to ndarray (CUDA support)\n",
    "os.environ[\"NEEDLE_BACKEND\"] = \"nd\"\n",
    "sys.path.append(\"./python\")\n",
    "\n",
    "import needle as ndl\n",
    "from needle import nn, optim\n",
    "from needle.data import DataLoader\n",
    "from needle.data.datasets.mnist_dataset import MNISTDataset\n",
    "\n",
    "# Check device\n",
    "device = ndl.cuda()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18498dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# Ensure data is in the 'data' folder\n",
    "train_dataset = MNISTDataset(\"./data/train-images-idx3-ubyte.gz\", \"./data/train-labels-idx1-ubyte.gz\")\n",
    "test_dataset = MNISTDataset(\"./data/t10k-images-idx3-ubyte.gz\", \"./data/t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a13e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device=None):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim, device=device, dtype=\"float32\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim, device=device, dtype=\"float32\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((x.shape[0], -1)) # Flatten\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "model = MLP(784, 256, 10, device=device)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67191fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Avg Loss: 0.3093\n",
      "Epoch 1/3, Avg Loss: 0.3093\n",
      "Epoch 2/3, Avg Loss: 0.1339\n",
      "Epoch 2/3, Avg Loss: 0.1339\n",
      "Epoch 3/3, Avg Loss: 0.0915\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.SoftmaxLoss()\n",
    "\n",
    "epochs = 3\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        batch_X = ndl.Tensor(batch_X, device=device)\n",
    "        batch_y = ndl.Tensor(batch_y, device=device)\n",
    "        \n",
    "        optimizer.reset_grad()\n",
    "        out = model(batch_X)\n",
    "        loss = loss_fn(out, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.numpy()\n",
    "        count += 1\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {total_loss/count:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Warmup\n",
    "    for i, (batch_X, batch_y) in enumerate(loader):\n",
    "        if i > 2: break\n",
    "        batch_X = ndl.Tensor(batch_X, device=device)\n",
    "        _ = model(batch_X)\n",
    "        \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for batch_X, batch_y in loader:\n",
    "        batch_X = ndl.Tensor(batch_X, device=device)\n",
    "        out = model(batch_X)\n",
    "        pred = out.numpy().argmax(axis=1)\n",
    "        correct += (pred == batch_y.numpy()).sum()\n",
    "        total += batch_y.shape[0]\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"duration\": duration,\n",
    "        \"throughput\": total/duration\n",
    "    }\n",
    "\n",
    "def estimate_memory(model):\n",
    "    \"\"\"\n",
    "    Estimates memory usage of model parameters.\n",
    "    For Float32: 4 bytes per element.\n",
    "    For Int8 Quantized: \n",
    "      - Int8 weights (1 byte)\n",
    "      - Scale (float32, 4 bytes)\n",
    "      - Zero Point (int8, 1 byte)\n",
    "      - Bias (float32, 4 bytes)\n",
    "    \"\"\"\n",
    "    total_bytes = 0\n",
    "    # Use _children() which returns all submodules recursively in Needle\n",
    "    modules = model._children()\n",
    "    \n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            bias = getattr(m, \"bias\", None)\n",
    "            bias_mem = bias.shape[0] * bias.shape[1] * 4 if bias is not None else 0\n",
    "            \n",
    "            # Check if quantized\n",
    "            if m.use_int8 and m._weight_q is not None:\n",
    "                # Quantized memory\n",
    "                q_mem = m._weight_q.memory_bytes()\n",
    "                total_bytes += q_mem + bias_mem\n",
    "            else:\n",
    "                # Float32 memory\n",
    "                w_mem = m.weight.shape[0] * m.weight.shape[1] * 4\n",
    "                total_bytes += w_mem + bias_mem\n",
    "                \n",
    "    return total_bytes / (1024 * 1024) # MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c939f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Float32\n",
    "print(\"Benchmarking Float32 Model...\")\n",
    "res_fp32 = benchmark_inference(model, test_loader, device)\n",
    "print(f\"Accuracy: {res_fp32['accuracy']:.4f}\")\n",
    "print(f\"Inference Time: {res_fp32['duration']:.4f} s\")\n",
    "print(f\"Throughput: {res_fp32['throughput']:.2f} samples/s\")\n",
    "\n",
    "mem_fp32 = estimate_memory(model)\n",
    "print(f\"Estimated Memory: {mem_fp32:.4f} MB\")\n",
    "\n",
    "# Quantize\n",
    "print(\"\\nQuantizing Model...\")\n",
    "model.eval()\n",
    "model.linear1.enable_quantization(axis=1)\n",
    "model.linear2.enable_quantization(axis=1)\n",
    "\n",
    "# Benchmark Int8\n",
    "print(\"\\nBenchmarking Int8 Model...\")\n",
    "res_int8 = benchmark_inference(model, test_loader, device)\n",
    "print(f\"Accuracy: {res_int8['accuracy']:.4f}\")\n",
    "print(f\"Inference Time: {res_int8['duration']:.4f} s\")\n",
    "print(f\"Throughput: {res_int8['throughput']:.2f} samples/s\")\n",
    "\n",
    "mem_int8 = estimate_memory(model)\n",
    "print(f\"Estimated Memory: {mem_int8:.4f} MB\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"Memory Reduction: {100 * (1 - mem_int8/mem_fp32):.2f}%\")\n",
    "print(f\"Speedup: {res_fp32['duration']/res_int8['duration']:.2f}x\")\n",
    "print(f\"Accuracy Drop: {100 * (res_fp32['accuracy'] - res_int8['accuracy']):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Systems-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
