{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needle Quantization Demo\n",
    "\n",
    "This notebook demonstrates the `int8` quantization support in Needle.\n",
    "We will cover:\n",
    "1. Basic quantization and dequantization of tensors.\n",
    "2. Quantized matrix multiplication.\n",
    "3. Dynamic and Static quantization of a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './python')\n",
    "import importlib\n",
    "import needle\n",
    "import needle.backend_ndarray.ndarray\n",
    "import needle.backend_ndarray\n",
    "import needle.backend_selection\n",
    "import needle.autograd\n",
    "\n",
    "importlib.reload(needle.backend_ndarray.ndarray)\n",
    "importlib.reload(needle.backend_ndarray)\n",
    "importlib.reload(needle.backend_selection)\n",
    "importlib.reload(needle.autograd)\n",
    "\n",
    "import needle as ndl\n",
    "print(f\"Needle file: {ndl.__file__}\")\n",
    "import numpy as np\n",
    "import needle.nn as nn\n",
    "from needle import backend_ndarray as nd\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from needle.backend_ndarray.ndarray import NDArray as NDArrayOrig\n",
    "from needle.autograd import NDArray as NDArrayAuto\n",
    "print(f\"NDArrayOrig: {NDArrayOrig}\")\n",
    "print(f\"NDArrayAuto: {NDArrayAuto}\")\n",
    "print(f\"Same? {NDArrayOrig is NDArrayAuto}\")\n",
    "\n",
    "# Check Tensor init logic\n",
    "t = ndl.Tensor([1, 2, 3], dtype=\"float32\")\n",
    "arr = t.realize_cached_data()\n",
    "print(f\"Array type: {type(arr)}\")\n",
    "print(f\"Array dtype: {arr.dtype}\")\n",
    "print(f\"Is instance of NDArrayAuto? {isinstance(arr, NDArrayAuto)}\")\n",
    "\n",
    "# Check quantize\n",
    "q = t.quantize_int8(1.0, 0)\n",
    "q_arr = q.realize_cached_data()\n",
    "print(f\"Quantized Array type: {type(q_arr)}\")\n",
    "print(f\"Quantized Array params: {q_arr._quant_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Quantization\n",
    "\n",
    "We can quantize a `float32` tensor to `int8` using `quantize_int8`.\n",
    "This stores the data as `int8` and attaches scale and zero_point metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a float32 tensor\n",
    "x = ndl.Tensor(np.random.randn(5, 5).astype(np.float32))\n",
    "print(\"Original Tensor (first row):\", x.numpy()[0])\n",
    "\n",
    "# Quantize to int8\n",
    "# We need to compute scale and zero_point first.\n",
    "# We can use the helper function from needle.quantization\n",
    "from needle.quantization import compute_scale_zero_point\n",
    "\n",
    "min_val, max_val = x.numpy().min(), x.numpy().max()\n",
    "scale, zp = compute_scale_zero_point(min_val, max_val)\n",
    "print(f\"Scale: {scale}, Zero Point: {zp}\")\n",
    "\n",
    "x_quant = x.quantize_int8(scale, zp)\n",
    "print(\"Quantized Tensor (int8 values):\", x_quant.realize_cached_data().numpy()[0])\n",
    "print(\"Quantized Tensor dtype:\", x_quant.dtype)\n",
    "\n",
    "# Dequantize back to float32\n",
    "x_dequant = x_quant.dequantize()\n",
    "print(\"Dequantized Tensor:\", x_dequant.numpy()[0])\n",
    "\n",
    "# Check error\n",
    "error = np.abs(x.numpy() - x_dequant.numpy()).max()\n",
    "print(f\"Max Quantization Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantized Matrix Multiplication\n",
    "\n",
    "We can perform matrix multiplication directly on `int8` tensors.\n",
    "The result is a `float32` tensor (dequantized output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two random matrices\n",
    "A = ndl.Tensor(np.random.randn(128, 128).astype(np.float32))\n",
    "B = ndl.Tensor(np.random.randn(128, 128).astype(np.float32))\n",
    "\n",
    "# Quantize them\n",
    "min_a, max_a = A.numpy().min(), A.numpy().max()\n",
    "scale_a, zp_a = compute_scale_zero_point(min_a, max_a)\n",
    "A_quant = A.quantize_int8(scale_a, zp_a)\n",
    "\n",
    "min_b, max_b = B.numpy().min(), B.numpy().max()\n",
    "scale_b, zp_b = compute_scale_zero_point(min_b, max_b)\n",
    "B_quant = B.quantize_int8(scale_b, zp_b)\n",
    "\n",
    "# Perform quantized matmul\n",
    "C_quant_out = A_quant @ B_quant\n",
    "\n",
    "# Perform standard float32 matmul\n",
    "C_ref = A @ B\n",
    "\n",
    "# Compare results\n",
    "print(\"Reference Output (first 5):\", C_ref.numpy().flatten()[:5])\n",
    "print(\"Quantized Output (first 5):\", C_quant_out.numpy().flatten()[:5])\n",
    "\n",
    "error = np.abs(C_ref.numpy() - C_quant_out.numpy()).mean()\n",
    "print(f\"Mean Absolute Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Quantization\n",
    "\n",
    "We will train a simple MLP on synthetic data, then apply Dynamic and Static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N = 1000\n",
    "input_dim = 32\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "\n",
    "X = np.random.randn(N, input_dim).astype(np.float32)\n",
    "# Random weights for ground truth\n",
    "W1 = np.random.randn(input_dim, hidden_dim).astype(np.float32) / np.sqrt(input_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim).astype(np.float32) / np.sqrt(hidden_dim)\n",
    "Y = np.maximum(0, X @ W1) @ W2 # ReLU activation\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "class SimpleDataset(ndl.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "dataset = SimpleDataset(X, Y)\n",
    "dataloader = ndl.data.DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# Define Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Train (briefly)\n",
    "opt = ndl.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.SoftmaxLoss() # Just using SoftmaxLoss as a dummy loss for regression-ish task (or MSE if available)\n",
    "# Wait, SoftmaxLoss expects integer labels usually?\n",
    "# Let's use MSE if available, or implement it.\n",
    "# nn_basic.py doesn't have MSELoss.\n",
    "# I'll just use a simple L2 loss manually.\n",
    "\n",
    "def l2_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true)**2).sum() / y_pred.shape[0]\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch, y_batch = batch\n",
    "        opt.reset_grad()\n",
    "        out = model(x_batch)\n",
    "        loss = l2_loss(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.numpy()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization\n",
    "\n",
    "We quantize the weights, but activations are quantized dynamically at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Quantize weights\n",
    "model.linear1.quantize_weights()\n",
    "model.linear2.quantize_weights()\n",
    "\n",
    "print(\"Weights quantized.\")\n",
    "\n",
    "# Measure inference time and error\n",
    "start_time = time.time()\n",
    "total_error = 0\n",
    "for batch in dataloader:\n",
    "    x_batch, y_batch = batch\n",
    "    out = model(x_batch)\n",
    "    # Compare with float32 forward (we need to disable quantization to compare? \n",
    "    # But we overwrote weights? No, quantize_weights stores quantized_weight separately.\n",
    "    # But forward uses quantized_weight if present.\n",
    "    # So we can't easily run float forward on the same model instance now without clearing quantized_weight.\n",
    "    # Let's just measure output against ground truth Y (which is what we trained on).\n",
    "    loss = l2_loss(out, y_batch)\n",
    "    total_error += loss.numpy()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Dynamic Quantization Inference Time: {end_time - start_time:.4f}s\")\n",
    "print(f\"Total L2 Error: {total_error / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantization\n",
    "\n",
    "We calibrate the model to find optimal scale/zero_point for activations, then run with static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from needle.quantization import calibrate\n",
    "\n",
    "# Calibrate\n",
    "print(\"Calibrating...\")\n",
    "calibrate(model, dataloader)\n",
    "print(\"Calibration complete.\")\n",
    "\n",
    "# Measure inference time and error\n",
    "start_time = time.time()\n",
    "total_error = 0\n",
    "for batch in dataloader:\n",
    "    x_batch, y_batch = batch\n",
    "    out = model(x_batch)\n",
    "    loss = l2_loss(out, y_batch)\n",
    "    total_error += loss.numpy()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Static Quantization Inference Time: {end_time - start_time:.4f}s\")\n",
    "print(f\"Total L2 Error: {total_error / len(dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
