{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3469d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using needle backend\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ndarray_backend_cpu' from partially initialized module 'needle.backend_ndarray' (most likely due to a circular import) (/Users/tyler/local-docs/CMU/DL-Systems-Project/python/needle/backend_ndarray/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./python\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneedle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mndl\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneedle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_ndarray \u001b[38;5;28;01mas\u001b[39;00m nd\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local-docs/CMU/DL-Systems-Project/python/needle/__init__.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optim\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local-docs/CMU/DL-Systems-Project/python/needle/backend_selection.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m BACKEND == \u001b[33m\"\u001b[39m\u001b[33mnd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing needle backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_ndarray \u001b[38;5;28;01mas\u001b[39;00m array_api\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_ndarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m         all_devices,\n\u001b[32m     13\u001b[39m         cuda,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         BackendDevice \u001b[38;5;28;01mas\u001b[39;00m Device,\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m     NDArray = array_api.NDArray\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local-docs/CMU/DL-Systems-Project/python/needle/backend_ndarray/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mndarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/local-docs/CMU/DL-Systems-Project/python/needle/backend_ndarray/ndarray.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndarray_backend_numpy\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ndarray_backend_cpu  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantParams, compute_quantization_params, validate_quant_params\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# math.prod not in Python 3.7\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ndarray_backend_cpu' from partially initialized module 'needle.backend_ndarray' (most likely due to a circular import) (/Users/tyler/local-docs/CMU/DL-Systems-Project/python/needle/backend_ndarray/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "\n",
    "import numpy as np\n",
    "import needle as ndl\n",
    "from needle import backend_ndarray as nd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Needle DL Framework - Quantization Demo\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3605b",
   "metadata": {},
   "source": [
    "## 1. Basic NDArray Quantization\n",
    "\n",
    "Let's start with basic quantization of NDArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affdabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a float32 array on CUDA\n",
    "np.random.seed(42)\n",
    "data_fp32 = np.random.randn(5, 4).astype(np.float32)\n",
    "arr_fp32 = nd.NDArray(data_fp32, device=ndl.cuda())\n",
    "\n",
    "print(\"Original Float32 Array:\")\n",
    "print(arr_fp32.numpy())\n",
    "print(f\"\\nDtype: {arr_fp32.dtype}\")\n",
    "print(f\"Shape: {arr_fp32.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize to uint8\n",
    "arr_uint8 = arr_fp32.quantize_uint8()\n",
    "\n",
    "print(\"Quantized UInt8 Array:\")\n",
    "print(arr_uint8.numpy())\n",
    "print(f\"\\nDtype: {arr_uint8.dtype}\")\n",
    "print(f\"Quantization Scale: {arr_uint8.quant_params.scale:.6f}\")\n",
    "print(f\"Quantization Zero Point: {arr_uint8.quant_params.zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e27cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dequantize back to float32\n",
    "arr_dequant = arr_uint8.dequantize()\n",
    "\n",
    "print(\"Dequantized Array (back to Float32):\")\n",
    "print(arr_dequant.numpy())\n",
    "\n",
    "# Calculate quantization error\n",
    "error = np.abs(arr_fp32.numpy() - arr_dequant.numpy())\n",
    "print(f\"\\nQuantization Error:\")\n",
    "print(f\"  Max Error: {error.max():.6f}\")\n",
    "print(f\"  Mean Error: {error.mean():.6f}\")\n",
    "print(f\"  RMS Error: {np.sqrt((error**2).mean()):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28111fd",
   "metadata": {},
   "source": [
    "## 2. Quantized Matrix Multiplication\n",
    "\n",
    "The key operation for neural networks is matrix multiplication. Let's compare float32 vs uint8 matmul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two matrices for multiplication on CUDA\n",
    "np.random.seed(123)\n",
    "A_np = np.random.randn(100, 50).astype(np.float32) * 2.0\n",
    "B_np = np.random.randn(50, 80).astype(np.float32) * 2.0\n",
    "\n",
    "A_fp32 = nd.NDArray(A_np, device=ndl.cuda())\n",
    "B_fp32 = nd.NDArray(B_np, device=ndl.cuda())\n",
    "\n",
    "print(f\"Matrix A shape: {A_fp32.shape}\")\n",
    "print(f\"Matrix B shape: {B_fp32.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c14ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float32 matmul\n",
    "start = time.time()\n",
    "C_fp32 = A_fp32 @ B_fp32\n",
    "time_fp32 = time.time() - start\n",
    "\n",
    "print(f\"Float32 Matmul:\")\n",
    "print(f\"  Result shape: {C_fp32.shape}\")\n",
    "print(f\"  Time: {time_fp32*1000:.4f} ms\")\n",
    "print(f\"  Sample output:\\n{C_fp32.numpy()[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b962cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize inputs\n",
    "A_uint8 = A_fp32.quantize_uint8()\n",
    "B_uint8 = B_fp32.quantize_uint8()\n",
    "\n",
    "print(f\"Quantized matrices:\")\n",
    "print(f\"  A dtype: {A_uint8.dtype}, scale: {A_uint8.quant_params.scale:.6f}\")\n",
    "print(f\"  B dtype: {B_uint8.dtype}, scale: {B_uint8.quant_params.scale:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32735805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized matmul (automatically uses uint8 kernel)\n",
    "start = time.time()\n",
    "C_uint8 = A_uint8 @ B_uint8\n",
    "time_uint8 = time.time() - start\n",
    "\n",
    "print(f\"UInt8 Quantized Matmul:\")\n",
    "print(f\"  Result shape: {C_uint8.shape}\")\n",
    "print(f\"  Result dtype: {C_uint8.dtype} (automatically converted to float32)\")\n",
    "print(f\"  Time: {time_uint8*1000:.4f} ms\")\n",
    "print(f\"  Sample output:\\n{C_uint8.numpy()[:3, :3]}\")\n",
    "\n",
    "# Compare\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Speedup: {time_fp32/time_uint8:.2f}x\")\n",
    "error = np.abs(C_fp32.numpy() - C_uint8.numpy())\n",
    "print(f\"  Max Error: {error.max():.6f}\")\n",
    "print(f\"  Mean Error: {error.mean():.6f}\")\n",
    "print(f\"  Relative Error: {(error.mean() / np.abs(C_fp32.numpy()).mean()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953d618",
   "metadata": {},
   "source": [
    "## 3. Tensor-Level Quantization\n",
    "\n",
    "Now let's test quantization at the Tensor level (higher-level API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on CUDA\n",
    "np.random.seed(456)\n",
    "x_np = np.random.randn(4, 3).astype(np.float32)\n",
    "x = ndl.Tensor(x_np, dtype=\"float32\", device=ndl.cuda())\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(x.numpy())\n",
    "print(f\"Dtype: {x.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize\n",
    "x_quant = x.quantize_uint8()\n",
    "\n",
    "print(\"Quantized Tensor:\")\n",
    "print(x_quant.numpy())\n",
    "print(f\"Dtype: {x_quant.dtype}\")\n",
    "\n",
    "# Dequantize\n",
    "x_dequant = x_quant.dequantize()\n",
    "\n",
    "print(\"\\nDequantized Tensor:\")\n",
    "print(x_dequant.numpy())\n",
    "\n",
    "error = np.abs(x.numpy() - x_dequant.numpy())\n",
    "print(f\"\\nMax error: {error.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807811df",
   "metadata": {},
   "source": [
    "## 4. Quantized Neural Network Layers\n",
    "\n",
    "The main application: quantized Linear layers for neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a943d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2-layer network on CUDA\n",
    "class SimpleNet(ndl.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = ndl.nn.Linear(10, 20, bias=True, device=ndl.cuda())\n",
    "        self.fc2 = ndl.nn.Linear(20, 5, bias=True, device=ndl.cuda())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = ndl.ops.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = SimpleNet()\n",
    "print(\"Network created with 2 linear layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fa1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input\n",
    "np.random.seed(789)\n",
    "x_np = np.random.randn(8, 10).astype(np.float32)\n",
    "x = ndl.Tensor(x_np, dtype=\"float32\")\n",
    "\n",
    "# Forward pass in training mode (float32)\n",
    "net.train()\n",
    "start = time.time()\n",
    "y_train = net(x)\n",
    "time_train = time.time() - start\n",
    "\n",
    "print(f\"Training Mode (Float32):\")\n",
    "print(f\"  Output shape: {y_train.shape}\")\n",
    "print(f\"  Time: {time_train*1000:.4f} ms\")\n",
    "print(f\"  Sample output:\\n{y_train.numpy()[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize weights for inference\n",
    "net.eval()\n",
    "net.fc1.quantize_weights()\n",
    "net.fc2.quantize_weights()\n",
    "\n",
    "print(\"Weights quantized!\")\n",
    "print(f\"  FC1 quantized: {net.fc1.quantized}\")\n",
    "print(f\"  FC2 quantized: {net.fc2.quantized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0340de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass with quantized weights\n",
    "start = time.time()\n",
    "y_quant = net(x)\n",
    "time_quant = time.time() - start\n",
    "\n",
    "print(f\"Inference Mode (Quantized UInt8):\")\n",
    "print(f\"  Output shape: {y_quant.shape}\")\n",
    "print(f\"  Time: {time_quant*1000:.4f} ms\")\n",
    "print(f\"  Sample output:\\n{y_quant.numpy()[:2]}\")\n",
    "\n",
    "# Compare\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Speedup: {time_train/time_quant:.2f}x\")\n",
    "error = np.abs(y_train.numpy() - y_quant.numpy())\n",
    "print(f\"  Max Error: {error.max():.6f}\")\n",
    "print(f\"  Mean Error: {error.mean():.6f}\")\n",
    "print(f\"  Relative Error: {(error / (np.abs(y_train.numpy()) + 1e-8)).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65664ed8",
   "metadata": {},
   "source": [
    "## 5. Memory Usage Comparison\n",
    "\n",
    "One of the key benefits of quantization is reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dadacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger matrices to see memory difference\n",
    "size = 1000\n",
    "large_fp32 = np.random.randn(size, size).astype(np.float32)\n",
    "large_uint8 = np.random.randint(0, 256, (size, size), dtype=np.uint8)\n",
    "\n",
    "mem_fp32 = large_fp32.nbytes / (1024 * 1024)  # MB\n",
    "mem_uint8 = large_uint8.nbytes / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"Matrix size: {size}x{size}\")\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  Float32:  {mem_fp32:.2f} MB\")\n",
    "print(f\"  UInt8:    {mem_uint8:.2f} MB\")\n",
    "print(f\"  Reduction: {mem_fp32/mem_uint8:.2f}x\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "types = ['Float32', 'UInt8']\n",
    "memory = [mem_fp32, mem_uint8]\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "\n",
    "bars = ax.bar(types, memory, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Memory Usage (MB)', fontsize=12)\n",
    "ax.set_title(f'Memory Usage Comparison\\n({size}x{size} Matrix)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(memory) * 1.2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, memory):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.2f} MB',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ 4x memory reduction achieved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e4d3b",
   "metadata": {},
   "source": [
    "## 6. Error Analysis Across Different Scales\n",
    "\n",
    "Let's analyze how quantization error varies with input magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test quantization error at different scales\n",
    "scales = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "max_errors = []\n",
    "mean_errors = []\n",
    "\n",
    "for scale in scales:\n",
    "    data = np.random.randn(100, 100).astype(np.float32) * scale\n",
    "    arr_fp32 = nd.NDArray(data)\n",
    "    arr_uint8 = arr_fp32.quantize_uint8()\n",
    "    arr_dequant = arr_uint8.dequantize()\n",
    "    \n",
    "    error = np.abs(arr_fp32.numpy() - arr_dequant.numpy())\n",
    "    max_errors.append(error.max())\n",
    "    mean_errors.append(error.mean())\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(scales, max_errors, 'o-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.set_xlabel('Input Scale', fontsize=12)\n",
    "ax1.set_ylabel('Max Quantization Error', fontsize=12)\n",
    "ax1.set_title('Max Error vs Input Scale', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax2.plot(scales, mean_errors, 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "ax2.set_xlabel('Input Scale', fontsize=12)\n",
    "ax2.set_ylabel('Mean Quantization Error', fontsize=12)\n",
    "ax2.set_title('Mean Error vs Input Scale', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Quantization error scales proportionally with input magnitude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f32d5",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Benefits of UInt8 Quantization:\n",
    "\n",
    "1. **Memory Efficiency**: 4x reduction (32 bits → 8 bits)\n",
    "2. **Performance**: Potential speedup from integer arithmetic\n",
    "3. **Energy Efficiency**: Lower power consumption\n",
    "4. **Deployment**: Better for edge devices and mobile\n",
    "\n",
    "### Tradeoffs:\n",
    "\n",
    "1. **Accuracy Loss**: Small quantization error (typically < 1%)\n",
    "2. **Limited Range**: 256 discrete values (vs continuous float32)\n",
    "3. **Implementation**: Requires careful scaling\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "- ✓ Inference on edge devices\n",
    "- ✓ Large model deployment\n",
    "- ✓ Memory-constrained environments\n",
    "- ✓ Production inference servers\n",
    "\n",
    "### When NOT to Use:\n",
    "\n",
    "- ✗ Training (need high precision gradients)\n",
    "- ✗ Tasks requiring high numerical precision\n",
    "- ✗ Small models where memory isn't an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ba1e2",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "This implementation provides:\n",
    "- ✓ NDArray-level quantization (quantize_uint8, dequantize, astype)\n",
    "- ✓ Tensor-level quantization API\n",
    "- ✓ Quantized matmul for uint8 inputs\n",
    "- ✓ Quantized Linear layers\n",
    "- ✓ Automatic dequantization for unsupported ops\n",
    "- ✓ NumPy backend support\n",
    "\n",
    "### Future Enhancements:\n",
    "1. C++/CUDA backend implementations for better performance\n",
    "2. Per-channel quantization for better accuracy\n",
    "3. Quantization-aware training (QAT)\n",
    "4. Dynamic quantization\n",
    "5. Support for other quantized operations (conv, etc.)\n",
    "6. Mixed precision (some layers quantized, others not)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for exploring the UInt8 quantization feature!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Systems-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
